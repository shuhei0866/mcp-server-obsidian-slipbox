import os
import re
from pathlib import Path
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ãŸè¨­å®š
DEFAULT_VAULT_PATH = "/path/to/your/vault"
vault_path_str = os.environ.get("OBSIDIAN_VAULT_PATH", DEFAULT_VAULT_PATH)
VAULT_ROOT = Path(vault_path_str)
INDEX_SAVE_DIR = Path(__file__).parent / "faiss_index"
CACHE_DIR = Path(__file__).parent / "embeddings_cache"


def extract_links(text):
    """Markdownã‹ã‚‰ [[ãƒªãƒ³ã‚¯å]] å½¢å¼ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ã™ã¹ã¦æŠ½å‡ºã™ã‚‹"""
    links = re.findall(r"\[\[(.*?)\]\]", text)
    clean_links = [link.split("|")[0] for link in links]
    return list(set(clean_links))


def create_index():
    print("--- SCRIPT START ---", flush=True)
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", flush=True)
        return

    print(f"ğŸ“‚ Vaultã‚’èª­ã¿è¾¼ã¿ä¸­: {VAULT_ROOT}", flush=True)

    # 1. Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    loader = DirectoryLoader(
        str(VAULT_ROOT),
        glob="**/*.md",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"},
    )

    documents = loader.load()
    print(f"ğŸ“„ {len(documents)} ä»¶ã®ãƒ¡ãƒ¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚", flush=True)

    # 1.5 ãƒªãƒ³ã‚¯æƒ…å ±ã®æŠ½å‡ºã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¸ã®ä»˜ä¸
    for doc in documents:
        links = extract_links(doc.page_content)
        doc.metadata["links"] = ", ".join(links) if links else ""

    # 2. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""],
    )

    chunks = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸  åˆè¨ˆ {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚", flush=True)

    # 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ™ã‚¯ãƒˆãƒ«åŒ–ã®è¨­å®š
    # ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹APIã‚³ãƒ¼ãƒ«ã¯ä¸€åº¦ã—ã‹ç™ºç”Ÿã—ã¾ã›ã‚“
    underlying_embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", chunk_size=100
    )

    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    store = LocalFileStore(str(CACHE_DIR))

    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings, store, namespace=underlying_embeddings.model
    )

    # 4. ãƒ™ã‚¯ãƒˆãƒ«åŒ– & ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    print(
        "ğŸ§  ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­ï¼ˆå¤‰æ›´ãŒãªã„ç®‡æ‰€ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã—ã¾ã™ï¼‰...", flush=True
    )
    vectorstore = FAISS.from_documents(chunks, cached_embedder)

    # 5. ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    INDEX_SAVE_DIR.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ä¸­: {INDEX_SAVE_DIR}", flush=True)
    vectorstore.save_local(str(INDEX_SAVE_DIR))

    print("\nâœ¨ å®Œäº†ï¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœ€æ–°ã®çŠ¶æ…‹ã«ãªã‚Šã¾ã—ãŸã€‚", flush=True)


if __name__ == "__main__":
    create_index()
